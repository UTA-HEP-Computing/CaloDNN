{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the LCD dataset analysis: The basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the step-by-step instructions below using jupyter notebook. Note this needs to be run from your DLKit/ directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CaloDNN Package Layout\n",
    "\n",
    "CaloDNN is a DLKit-based package used for studying LCD Calorimetry with deep neural networks. The training and analysis for the classification problem are done by running an \"experiment\" using the CaloDNN package and Jupyter notebooks (it is also possible to run on the command line).  \n",
    "\n",
    "When an experiment is run there are four steps that are done:\n",
    "    1. Loading the dataset\n",
    "    2. Load a pre-existing model or build your own model\n",
    "    3. Train the model\n",
    "    4. Run the analysis\n",
    "The package allows you to turn on or off each of these steps as needed using command-line arguments that are described in the next step.\n",
    "\n",
    "The package consists of the following primary python files:\n",
    "\n",
    "*CaloDNN/ClassificationExperiment.py*: This is the “experiment” that drives everything.\n",
    "\n",
    "*CaloDNN/ClassificationArguments.py*: This is the file where all of the above arguments are defined and parsed. You can add your own options here if need be. Some defaults are defined here.\n",
    "\n",
    "*CaloDNN/ClassificationScanConfig.py*: This is the configuration file. The model and experiment parameters are set here. This example is setup to allow hyper-parameter scanning. It also contains the list of input files and maps the files and datasets to classes as well as controls what variables are used in the Neural Network.\n",
    "\n",
    "*CaloDNN/Models.py*: This contains the Keras models, wrapped in a DLKit ModelWrapper class.\n",
    "\n",
    "*CaloDNN/LCDData.py*: Contains the DLGenerators to read the data.\n",
    "\n",
    "The basic analysis package is the CaloDNN/ClassificationExperiment.py. You can run this with the `--help` switch to see the command-line options:  \n",
    "\n",
    "(Running in Jupyter Notebooks: click on the box below and hit shift-enter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ClassificationExperiment.py [-h] [-C CONFIG] [-L LOADMODEL]\n",
      "                                   [--gpu GPUID] [--cpu] [--NoTrain]\n",
      "                                   [--NoAnalysis] [--NoModel] [--LowMem]\n",
      "                                   [--Test] [--Recover] [-s HYPERPARAMSET]\n",
      "                                   [--nocache] [--preload] [-r RUNNINGTIME]\n",
      "                                   [-p] [--GracefulExit]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -C CONFIG, --config CONFIG\n",
      "                        Use specified configuration file.\n",
      "  -L LOADMODEL, --LoadModel LOADMODEL\n",
      "                        Loads a model from specified directory.\n",
      "  --gpu GPUID           Use specified GPU.\n",
      "  --cpu                 Use CPU.\n",
      "  --NoTrain             Do not run training.\n",
      "  --NoAnalysis          Do not run analysis.\n",
      "  --NoModel             Do not build or load model.\n",
      "  --LowMem              Minimize Memory Usage.\n",
      "  --Test                Run in test mode (reduced examples and epochs).\n",
      "  --Recover             Train only if fail to load model (use with --NoTrain\n",
      "                        and --Load or --LoadPrevious).\n",
      "  -s HYPERPARAMSET, --hyperparamset HYPERPARAMSET\n",
      "                        Use specificed (by index) hyperparameter set.\n",
      "  --nocache             Do not use cache data to disk for faster read > 1st\n",
      "                        epoch.\n",
      "  --preload             Preload the data into memory. Caution: requires lots\n",
      "                        of memory.\n",
      "  -r RUNNINGTIME, --runningtime RUNNINGTIME\n",
      "                        End training after specified number of seconds.\n",
      "  -p, --LoadPrevious    Load the last trained model.\n",
      "  --GracefulExit        Enable graceful exit via Ctrl-C or SIGTERM signal.\n"
     ]
    }
   ],
   "source": [
    "%run -im CaloDNN.ClassificationExperiment -- --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command-line options above have a good explanation beside them. The default configuration file can be found at CaloDNN/ClassificationScanConfig.py and is explained in the next section.\n",
    "\n",
    "\n",
    "#### Configuration and Hyperparameter Scanning\n",
    "\n",
    "The DNN architecture is defined by the types, number, and dimension of layers. Hyper-parameter\n",
    "scanning refers to the process of searching for an optimal architecture that performs well for a task and can be trained and applied within reasonable time. Beyond the parameters that define the DNN architecture, other configuration parameters allow setting and testing activation and cost functions, optimization (e.g. minimization) techniques, and rate other training parameters.\n",
    "\n",
    "In the DLKit, these parameters are set in a configuration file, which defines a single python key/value dictionary. DLKit puts the contents of this dictionary in the global scope with the\n",
    "keys as the variable names. For the CaloDNN package an example is `CaloDNN/ClassificationScanConfig.py`:\n",
    "\n",
    "Some of the key configuration parameters are shown below:\n",
    "\n",
    "```\n",
    "Config={\n",
    "    \"MaxEvents\":int(3.e6),\n",
    "    \"NTestSamples\":100000,\n",
    "    \"NClasses\":4,\n",
    "\n",
    "    \"Epochs\":1000,\n",
    "    \"BatchSize\":1024,\n",
    "\n",
    "...\n",
    "\n",
    "    # Configure Running time callback\n",
    "    # Set RunningTime to a value to stop training after N seconds.\n",
    "    \"RunningTime\": 2*3600,\n",
    "\n",
    "    # Load last trained version of this model configuration. (based on Name var below)\n",
    "    \"LoadPreviousModel\":True\n",
    "    }\n",
    "```\n",
    "\n",
    "An important parameter in this configuration file is the `RunningTime`, which sets duration of the training. Using this parameter, you can train a model for a fixed amount of time. You can rerun the job to continue training, which will automatically load the last successful training session, as set by `LoadPreviousModel` parameter.\n",
    "\n",
    "[`CaloDNN/ClassificationScanConfig.py`](https://github.com/UTA-HEP-Computing/CaloDNN/blob/master/ClassificationScanConfig.py) is well commented. We suggest you read through the comments.\n",
    "\n",
    "The following lines:\n",
    "\n",
    "```\n",
    "# Parameters to scan and their scan points.\n",
    "Params={    \"optimizer\":[\"'RMSprop'\",\"'Adam'\",\"'SGD'\"],\n",
    "            \"Width\":[32,64,128,256,512],\n",
    "            \"Depth\":range(1,5),\n",
    "            \"lr\":[0.01,0.001],\n",
    "            \"decay\":[0.01,0.001],\n",
    "          }\n",
    " ```\n",
    "\n",
    "will generate 3 x 5 x 4 x 2 x 2 = 240 different configurations options, which we can test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Test Classification Problem\n",
    "\n",
    "We will now run a test classification experiment using the --Test flag on the command line. In this mode there are a reduced number of events and epochs run. This is a good test of your setup and to walk-through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU \n",
      "Found 8 CPUs and 2 GPUs. Using 4 threads. max_threads = 12\n",
      "HyperParameter Scan:  240 possible combiniations.\n",
      "______________________________________\n",
      "ScanConfiguration\n",
      "______________________________________\n",
      "Picked combination:  0\n",
      "Combo[0]={'Width': 32, 'Depth': 1, 'lr': 0.01, 'optimizer': \"'RMSprop'\", 'decay': 0.01}\n",
      "Model Filename:  CaloDNN_32_1_0.01_RMSprop_0.01\n",
      "______________________________________\n",
      "Test Mode: Set MaxEvents to 20000 and Epochs to 10\n",
      "Searching in : /data/LCD/*/*.h5\n",
      "Found 639 files.\n",
      "Train Class Index Map: {'Pi0': 0, 'ChPi': 1, 'Gamma': 2, 'Ele': 3}\n",
      "Caching data on disk for faster processing after first epoch. Hope you have enough disk space.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named merge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python2.7/runpy.pyc\u001b[0m in \u001b[0;36mrun_module\u001b[0;34m(mod_name, init_globals, run_name, alter_sys)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malter_sys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         return _run_module_code(code, init_globals, run_name,\n\u001b[0;32m--> 188\u001b[0;31m                                 fname, loader, pkg_name)\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Leave the sys module alone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/runpy.pyc\u001b[0m in \u001b[0;36m_run_module_code\u001b[0;34m(code, init_globals, mod_name, mod_fname, mod_loader, pkg_name)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mmod_globals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         _run_code(code, mod_globals, init_globals,\n\u001b[0;32m---> 82\u001b[0;31m                   mod_name, mod_fname, mod_loader, pkg_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Copy the globals of the temporary module, as they\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# may be cleared when the temporary module goes away\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/runpy.pyc\u001b[0m in \u001b[0;36m_run_code\u001b[0;34m(code, run_globals, init_globals, mod_name, mod_fname, mod_loader, pkg_name)\u001b[0m\n\u001b[1;32m     70\u001b[0m                        \u001b[0m__loader__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                        __package__ = pkg_name)\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mexec\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_globals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun_globals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lheelan/FermiWorkshop_GitDoc/DLKit/CaloDNN/ClassificationExperiment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Build/Load the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDLTools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelWrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCaloDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# You can automatically load the latest previous training of this model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lheelan/FermiWorkshop_GitDoc/DLKit/CaloDNN/Models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDLTools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelWrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named merge"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%run -im CaloDNN.ClassificationExperiment -- --Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the evolution of the analysis as a function of epochs (in this test we are only running 20k events with 10 epochs. When we run an experiment the model is saved to CaloDNN/TrainedModels, so you can re-load the model in future analyses or further experiments with more events and epochs. The naming of the model reflects the hyper-parameter settings. \n",
    "\n",
    "At the end you will see a plot reflecting the 'success' of the model at classifying each of the four types of particles (electron, photon, charged or neutral pions). The 'area' (area under the curve) gives a measure of how well this hyper-parameter scan was overall at classifying each particle type (the larger the number the better!). In the next part we will test different models to see which gives the best rate of successfully classifying these events.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
